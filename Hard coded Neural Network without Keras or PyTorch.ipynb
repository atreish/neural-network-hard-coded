{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import  matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>Klength</th>\n",
       "      <th>Kwidth</th>\n",
       "      <th>Asym_coeff</th>\n",
       "      <th>length of kernel groove</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.26</td>\n",
       "      <td>14.84</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>5.763</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.221</td>\n",
       "      <td>5.220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.88</td>\n",
       "      <td>14.57</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>5.554</td>\n",
       "      <td>3.333</td>\n",
       "      <td>1.018</td>\n",
       "      <td>4.956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.29</td>\n",
       "      <td>14.09</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>5.291</td>\n",
       "      <td>3.337</td>\n",
       "      <td>2.699</td>\n",
       "      <td>4.825</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.84</td>\n",
       "      <td>13.94</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>5.324</td>\n",
       "      <td>3.379</td>\n",
       "      <td>2.259</td>\n",
       "      <td>4.805</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.14</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.562</td>\n",
       "      <td>1.355</td>\n",
       "      <td>5.175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area  Perimeter  Compactness  Klength  Kwidth  Asym_coeff  \\\n",
       "0  15.26      14.84       0.8710    5.763   3.312       2.221   \n",
       "1  14.88      14.57       0.8811    5.554   3.333       1.018   \n",
       "2  14.29      14.09       0.9050    5.291   3.337       2.699   \n",
       "3  13.84      13.94       0.8955    5.324   3.379       2.259   \n",
       "4  16.14      14.99       0.9034    5.658   3.562       1.355   \n",
       "\n",
       "   length of kernel groove  class_label  \n",
       "0                    5.220            1  \n",
       "1                    4.956            1  \n",
       "2                    4.825            1  \n",
       "3                    4.805            1  \n",
       "4                    5.175            1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/atreish/AIM_5007_Neural_Networks_-_Deep_Learning/main/seeds_dataset.txt',\n",
    "                   sep='\\s+',header=None, names = ['Area','Perimeter','Compactness','Klength','Kwidth','Asym_coeff','length of kernel groove','class_label'])\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize Numeric Data, Not the label column\n",
    "data.iloc[:,:-1] = data.iloc[:,:-1].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "#Shuffle the dataset to ensure the network is trained on all 3 available classes\n",
    "data1 = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#Change X  (numeric columns) and y (class columns) to arrays and the target datatype to int\n",
    "X = np.array(data1)[:, :-1]\n",
    "y = np.array(data1)[:, -1].astype(int)\n",
    "\n",
    "#Apply one hot encoding for each class, this will be our target class \n",
    "target_layer = []\n",
    "for i in y:\n",
    "    if i == 1:\n",
    "        target_layer.append([1,0,0])\n",
    "    elif i == 2:\n",
    "        target_layer.append([0,1,0])\n",
    "    else:\n",
    "        i == 3\n",
    "        target_layer.append([0,0,1])\n",
    "        \n",
    "target_layer = np.array(target_layer)\n",
    "\n",
    "#Train-test split 150/60\n",
    "X_train = X[:150]\n",
    "X_test = X[150:]\n",
    "y_train = target_layer[:150]\n",
    "y_test = target_layer[150:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network Functions for [7,7,3] Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Activation Function for this network'''\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "'''Append the entire matrix with a column of ones for the bias node'''\n",
    "\n",
    "def append(X):\n",
    "    height = X.shape[0] \n",
    "    bias_column = np.ones((height,1))\n",
    "    X_appended = np.hstack((X,bias_column))\n",
    "    return X_appended\n",
    "\n",
    "'''Initializes a Weight Matrix from the input layer to the hidden layer using the length of a single row \n",
    "for argument Matrix X'''\n",
    "\n",
    "def w_input(X):\n",
    "    X_appended = append(X)\n",
    "    rows = len(X[0]) \n",
    "    cols = len(X[0])\n",
    "    W = np.random.sample((cols, rows)) - 0.5\n",
    "    random_bias_weight = np.random.sample()\n",
    "    bias_row = np.repeat(random_bias_weight, rows)\n",
    "    W_input = np.insert(W, cols, bias_row, axis=0)\n",
    "    return W_input\n",
    "\n",
    "'''This is the dot product between a weight matrix and the appended X matrix (one row at a time), this returns\n",
    "an unactivated hidden layer of length 7, as specified by the dimensions of the problem'''\n",
    "\n",
    "def h_raw(X,i,weight1=None): \n",
    "    if weight1 is None:\n",
    "        weight1 = w_input(X)\n",
    "    X_appended_row = append(X)[i]\n",
    "    hraw = np.dot(X_appended_row, weight1)\n",
    "    return hraw, weight1\n",
    "\n",
    "'''This function now activates the hidden layer by applying the sigmoid activation function and returns the \n",
    "weight that was used to get from the input layer to the hidden layer'''\n",
    "\n",
    "def h_activated(X,i,weight1=None):\n",
    "    hidden_layer_raw, weight1 = h_raw(X,i,weight1)\n",
    "    hactivated = sigmoid(hidden_layer_raw)\n",
    "    return hactivated, weight1\n",
    "\n",
    "'''Initializes a Weight Matrix for the hidden to output layer using the length of the activated hidden layer,\n",
    "this also appends a random weight at the bottom of the matrix for the bias from hidden to output layer'''\n",
    "\n",
    "def w_output(X, i, y_train, weight1=None):\n",
    "    hactivated, _ = h_activated(X,i,weight1=weight1)\n",
    "    rows = len(hactivated)\n",
    "    W = np.random.sample((rows, 3)) - 0.5\n",
    "    random_weight = np.random.sample([1]) - 0.5\n",
    "    weight_vector = np.repeat(random_weight,3)\n",
    "    W_output = np.insert(W, rows, weight_vector, axis=0)\n",
    "    return W_output\n",
    "\n",
    "'''This is the dot product between an appended activated hidden layer and W_output, this gives us the first\n",
    "output layer values'''\n",
    "\n",
    "def o_raw(X,i, weight1=None, weight2=None):\n",
    "    hactivated, weight1 = h_activated(X, i , weight1)\n",
    "    h_activated_with_bias = np.append(hactivated,[1]) \n",
    "    if weight2 is None:\n",
    "        weight2 = w_output(X,i,weight1)\n",
    "    oraw = np.dot(h_activated_with_bias, weight2)\n",
    "    return oraw, weight2\n",
    "\n",
    "'''This function now activates the output layer by applying the sigmoid activation function and returns the \n",
    "weight that was used to get from the hidden layer to the output layer'''\n",
    "\n",
    "def o_activated(X,i, weight1=None, weight2=None):\n",
    "    oraw, weight2 = o_raw(X,i, weight1=weight1, weight2=weight2)\n",
    "    oactivated = sigmoid(oraw)\n",
    "    return oactivated, weight2\n",
    "\n",
    "'''This Error function determines the error of the output for the i-th row of the Matrix and the i-th target layer\n",
    "and outputs a single value that will be used in the backpropogation steps'''\n",
    "\n",
    "def error(X,y,i,weight1=None, weight2=None):\n",
    "    oactivated, _ = o_activated(X, i, weight1=weight1, weight2=weight2)\n",
    "    error = .5*((y[i] - oactivated)**2).sum()\n",
    "    return error\n",
    "\n",
    "'''The Feedforward Steps all conglomerated into one function from X matrix row to output activated. This catch-all\n",
    "function will return the outputlayer actived for the i-th row, the weight matricies used form both the input layer\n",
    "to hidden layer and from the hidden layer to the input layer and the associated erro'''\n",
    "\n",
    "def feedforward(X, y, i, weight1=None, weight2=None):\n",
    "    hactivated, weight1 = h_activated(X,i, weight1)\n",
    "    oactivated, weight2 = o_activated(X,i, weight1=weight1, weight2=weight2)\n",
    "    err = error(X,y,i,weight1=weight1, weight2=weight2)\n",
    "    return oactivated, hactivated, err, weight1, weight2\n",
    "\n",
    "'''First Step in Backpropogation, this produces the Gradient Matrix from the Hidden Layer to the\n",
    "Output Layer (8x3)'''\n",
    "\n",
    "def grad_hidden_to_output(X,y,i, oactivated, hactivated): \n",
    "    E = (oactivated - y[i]) * oactivated * (1 - oactivated)\n",
    "    h_activated_transpose = np.array([[i] for i in hactivated])\n",
    "    grad_hidden_to_output = h_activated_transpose * E \n",
    "    bias_gradient = ((oactivated - y[i]) * oactivated * (1 - oactivated)).sum()\n",
    "    bias_h2o =  np.array([bias_gradient])\n",
    "    len_bias_row = grad_hidden_to_output.shape[1]\n",
    "    BG_H_O = np.repeat(bias_h2o[0],len_bias_row)\n",
    "    hidden_to_output_gradient_matrix = np.vstack([grad_hidden_to_output, BG_H_O])\n",
    "    return hidden_to_output_gradient_matrix\n",
    "\n",
    "'''The Gradient Matrix from the Input Layer to the Hidden Layer (8x7)'''\n",
    "\n",
    "def grad_input_to_hidden(X, y, i, oactivated, hactivated, W_output):\n",
    "    input_layer = X[i]\n",
    "    target_layer = y[i]\n",
    "    W_output_wo_bias = W_output[:-1,:] \n",
    "    E = (oactivated - target_layer) * oactivated * (1 - oactivated) \n",
    "    grad_input_to_hidden = np.zeros((7,7))\n",
    "    for j in range(len(hactivated)):\n",
    "        pt1 = []  \n",
    "        pt2 = []\n",
    "        pt1.append( np.dot(E, W_output_wo_bias[j]) ) \n",
    "        pt2.append( hactivated[j] * (1 - hactivated[j])) \n",
    "        grad_input_to_hidden[:,j] = np.array(pt1) * np.array(pt2) * input_layer\n",
    "    pt3 = np.dot(E, W_output_wo_bias.T)\n",
    "    pt4 = hactivated *(1-hactivated)\n",
    "    bias_gradient_il = np.dot(pt3,pt4)\n",
    "    len_bias_row = hactivated.shape[0]\n",
    "    BG_I_H = np.repeat(bias_gradient_il, len_bias_row)\n",
    "    input_to_hidden_gradient_matrix = np.vstack((grad_input_to_hidden, BG_I_H))\n",
    "    return input_to_hidden_gradient_matrix\n",
    "\n",
    "'''Updated Weights from the Input to Hidden Layer with learning rate lr'''\n",
    "\n",
    "def update_input_hidden_weights(X,y,i,lr, oactivated, hactivated, weight1=None, weight2=None):\n",
    "    input_to_hidden_gradient_matrix = grad_input_to_hidden(X, y, i, oactivated, hactivated, weight2)\n",
    "    updated_weights_input_hidden = weight1 - lr * input_to_hidden_gradient_matrix\n",
    "    return updated_weights_input_hidden\n",
    "\n",
    "'''Updated Weights for Hidden to Output Layer with lr'''\n",
    "\n",
    "def update_hidden_output_weights(X,y,i,lr, oactivated, hactivated, weight2=None):\n",
    "    hidden_to_output_gradient_matrix =  grad_hidden_to_output(X,y,i, oactivated, hactivated)\n",
    "    updated_weights_hidden_output = weight2 - lr * hidden_to_output_gradient_matrix\n",
    "    return updated_weights_hidden_output\n",
    "\n",
    "'''Backpropogation Function'''\n",
    "\n",
    "def backpropogation(X,y,i,lr,oactivated,hactivated,weight1=None, weight2=None):\n",
    "    updated_weight1 = update_input_hidden_weights(X,y,i,lr, oactivated, hactivated, weight1=weight1, weight2=weight2)\n",
    "    updated_weight2 = update_hidden_output_weights(X,y,i,lr, oactivated, hactivated, weight2)\n",
    "    oactivated1, hactivated1, err, updated_weight1, updated_weight2 = feedforward(X, y, i, weight1=updated_weight1, weight2=updated_weight2)\n",
    "    return err, oactivated1, updated_weight1, updated_weight2\n",
    "\n",
    "'''Transforms the Rows of all oactivated1 to one hot encoding to compare with y_trian'''\n",
    "\n",
    "def output_encoder(Matrix):\n",
    "    for row in Matrix:\n",
    "        row[row.argmax(0)] = 1\n",
    "        row[row < 1] = 0\n",
    "    return Matrix\n",
    "\n",
    "'''Creates a visualization of the Errors'''\n",
    "\n",
    "def error_plotter(Matrix,threshold): \n",
    "    result = np.array(list(map(sum, Matrix))) / len(Matrix[0])\n",
    "    index = np.array(list(range(len(result))))\n",
    "    Ezz = np.vstack((index+1, result)).T\n",
    "    x = Ezz[:,0]\n",
    "    y = Ezz[:,1]\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(x,y)\n",
    "    plt.axhline(y=threshold, color='r', linestyle='--')\n",
    "    plt.show\n",
    "\n",
    "'''This function returns the number of EPOCHS needed to reach convergence to the error threshold'''\n",
    "    \n",
    "def epochs_for_convergence(avg_error, threshold, X):\n",
    "    for i in range(len(avg_error)):\n",
    "        if avg_error[i] < threshold:\n",
    "            count = i / len(X)\n",
    "            return math.ceil(count)\n",
    "        \n",
    "def epochs_for_mean_convergence(normalized_error, threshold):\n",
    "    for i in range(len(normalized_error)):\n",
    "        if normalized_error[i] < threshold:\n",
    "            count = i\n",
    "            return math.ceil(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT 1: Different Learning Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Using the given Neural Network functions and using the Sigmoid Activation function we will use the Seeds Dataset to experiment with different learning rates. The following NN_traing() function uses Stochastic Gradient\n",
    "Descent and updates the weights one observation at a time for all given epochs. That means the the weights will\n",
    "be updated (150 x n_epochs) and output as a result. This function will also provide us with Mean error for the \n",
    "Final epoch, the earlist epoch that witnesses the error of a single observation dip below our given threshold,\n",
    "and the epoch in which the mean error dips below the given threshold.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_training(X, y, lr, threshold=.001, epoch_count=1000):\n",
    "    wt1 = None\n",
    "    wt2 = None\n",
    "    final_output = []\n",
    "    errors = [] \n",
    "    avg_error = [] \n",
    "    for n in range(epoch_count):   \n",
    "        M = np.concatenate((X, y), axis=1)\n",
    "        M = pd.DataFrame(M)\n",
    "        M1 = M.sample(frac=1).reset_index(drop=True)\n",
    "        XX_train = np.array(M1.iloc[:,:7])\n",
    "        yy_train = np.array(M1.iloc[:,7:])\n",
    "              \n",
    "        for i in range(XX_train.shape[0]):  \n",
    "            oact, hact, err, wt1, wt2 = feedforward(XX_train,yy_train,i,weight1=wt1, weight2=wt2)\n",
    "            err, oactivated1, updated_weight1, updated_weight2 = backpropogation(XX_train,yy_train,i,lr,oactivated=oact,hactivated=hact,weight1=wt1, weight2=wt2)\n",
    "            wt1 = updated_weight1\n",
    "            wt2 = updated_weight2\n",
    "            final_output.append(oactivated1)\n",
    "            errors.append(err)\n",
    "            avg_error.append(err)\n",
    "    \n",
    "    epoch_errors = np.reshape(errors,(epoch_count, len(X)))\n",
    "    normalized_errors = np.array(list(map(sum,  epoch_errors))) / len( epoch_errors[0])\n",
    "    \n",
    "    epochs_single_convergence = epochs_for_convergence(avg_error, threshold, X)\n",
    "    epochs_mean_convergence = epochs_for_mean_convergence(normalized_errors, threshold)\n",
    "    \n",
    "    output = np.asarray(final_output[-len(X):])\n",
    "    encoded_output = output_encoder(output)      \n",
    "    tot = np.sum(np.all(encoded_output == yy_train, axis=1))\n",
    "    accuracy = tot/len(y) \n",
    "    \n",
    "    print('-------------------------------------------------------------------------------------------------')\n",
    "    print('After {} epochs, and a learning rate of {} the model accuracy is {}%'.format(epoch_count,lr,round(accuracy*100, 2)))\n",
    "    print('Mean Error of the Final Epoch = {}'.format(round(normalized_errors[-1],5)))\n",
    "    print('The Number of Epochs for a single activated output layer error to dip below the threshold is {}'.format(epochs_single_convergence))\n",
    "    print('The Number of Epochs for the Mean Epoch Error to dip below the threshold is {}'.format(epochs_mean_convergence))\n",
    "    \n",
    "    return wt1, wt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``This testing function will now provide the accuracy of a single feedforward pass for all observations using the\n",
    "obtained weights for each learning rate``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_testing(X, y,wt1,wt2,lr):\n",
    "    final_output = []\n",
    "    errors = [] \n",
    "    for i in range(X.shape[0]):  \n",
    "        oact, hact, err, wt1, wt2 = feedforward(X,y,i,weight1=wt1, weight2=wt2)\n",
    "        final_output.append(oact)\n",
    "        errors.append(err)\n",
    "    output = np.asarray(final_output[-len(X):])\n",
    "    encoded_output = output_encoder(output)      \n",
    "    tot = np.sum(np.all(encoded_output == y_test, axis=1))\n",
    "    accuracy = tot/len(y_test)\n",
    "    print('\\nAccuracy using updated weights from training set with lr={} is {}%'.format(lr,round(accuracy*100, 2)))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```The following is our actual experiment where we assign one out of a list of learning rates and capture their output updated weight matricies. We also observe their final Epoch Mean Error, accuracy, the number of epochs for both a single observation's error and the mean epoch error to dip below the threshold. In our previous experiments, we obtained over 92% accuracy on testing data after 3000 epochs, and the mean epoch error dips below\n",
    "the threshold before epoch 1000. Here we will use 2500 epochs. Finally, the captured weights are tested on our validation set to determine the accuracy obtained for each learning rate.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------\n",
      "After 2500 epochs, and a learning rate of 0.001 the model accuracy is 88.67%\n",
      "Mean Error of the Final Epoch = 0.16471\n",
      "The Number of Epochs for a single activated output layer error to dip below the threshold is None\n",
      "The Number of Epochs for the Mean Epoch Error to dip below the threshold is None\n",
      "-------------------------------------------------------------------------------------------------\n",
      "After 2500 epochs, and a learning rate of 0.05 the model accuracy is 98.0%\n",
      "Mean Error of the Final Epoch = 0.01934\n",
      "The Number of Epochs for a single activated output layer error to dip below the threshold is 61\n",
      "The Number of Epochs for the Mean Epoch Error to dip below the threshold is None\n",
      "-------------------------------------------------------------------------------------------------\n",
      "After 2500 epochs, and a learning rate of 0.1 the model accuracy is 100.0%\n",
      "Mean Error of the Final Epoch = 0.00709\n",
      "The Number of Epochs for a single activated output layer error to dip below the threshold is 33\n",
      "The Number of Epochs for the Mean Epoch Error to dip below the threshold is 1738\n",
      "-------------------------------------------------------------------------------------------------\n",
      "After 2500 epochs, and a learning rate of 0.2 the model accuracy is 100.0%\n",
      "Mean Error of the Final Epoch = 0.00237\n",
      "The Number of Epochs for a single activated output layer error to dip below the threshold is 15\n",
      "The Number of Epochs for the Mean Epoch Error to dip below the threshold is 708\n",
      "-------------------------------------------------------------------------------------------------\n",
      "After 2500 epochs, and a learning rate of 0.6 the model accuracy is 100.0%\n",
      "Mean Error of the Final Epoch = 0.0002\n",
      "The Number of Epochs for a single activated output layer error to dip below the threshold is 7\n",
      "The Number of Epochs for the Mean Epoch Error to dip below the threshold is 138\n",
      "-------------------------------------------------------------------------------------------------\n",
      "After 2500 epochs, and a learning rate of 0.9 the model accuracy is 100.0%\n",
      "Mean Error of the Final Epoch = 0.00012\n",
      "The Number of Epochs for a single activated output layer error to dip below the threshold is 5\n",
      "The Number of Epochs for the Mean Epoch Error to dip below the threshold is 168\n",
      "-------------------------------------------------------------------------------------------------\n",
      "After 2500 epochs, and a learning rate of 0.999 the model accuracy is 100.0%\n",
      "Mean Error of the Final Epoch = 7e-05\n",
      "The Number of Epochs for a single activated output layer error to dip below the threshold is 3\n",
      "The Number of Epochs for the Mean Epoch Error to dip below the threshold is 102\n",
      "\n",
      "Accuracy using updated weights from training set with lr=0.001 is 93.33%\n",
      "\n",
      "Accuracy using updated weights from training set with lr=0.05 is 93.33%\n",
      "\n",
      "Accuracy using updated weights from training set with lr=0.1 is 96.67%\n",
      "\n",
      "Accuracy using updated weights from training set with lr=0.2 is 96.67%\n",
      "\n",
      "Accuracy using updated weights from training set with lr=0.6 is 95.0%\n",
      "\n",
      "Accuracy using updated weights from training set with lr=0.9 is 96.67%\n",
      "\n",
      "Accuracy using updated weights from training set with lr=0.999 is 96.67%\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [.001,.05,.1,.2,.6,.9, .999]\n",
    "W1 = []\n",
    "W2 = []\n",
    "for i in learning_rates:\n",
    "    wt1, wt2 = nn_training(X_train, y_train, i, threshold=.01, epoch_count=2500)\n",
    "    W1.append(wt1)\n",
    "    W2.append(wt2)\n",
    "\n",
    "for i in range(len(W1)):\n",
    "    nn_testing(X_test, y_test, W1[i], W2[i], lr=learning_rates[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```The above results show that the learning rates from .001 - .999 have progressively better results for training accuracy and the faster to convergence to the error treshold for single instance and for epoch mean error however .6 has an epoch mean error that converges faster than .9 but .999 is fastest as it converges to the threshold for single instance in 3 epochs, epoch mean error in 102 epochs and has the best testing rate accuracy (shared with .1,.2,.9)  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  EXPERIMENT 2: Different Activation Function Utilizing the Hyperbolic Tangent as the Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```In this experiment, the activation function for the Newtwork will be the hyperbolic tangent. This is a common activation function. In this section, the h_activated(), o_activated(), gradient functions are edited since the gradient matricies require that the derivitive of the activation function be used.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Activation Function for this network'''\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "'''This function now activates the hidden layer by applying the sigmoid activation function and returns the \n",
    "weight that was used to get from the input layer to the hidden layer'''\n",
    "\n",
    "def h_activated(X,i,weight1=None):\n",
    "    hidden_layer_raw, weight1 = h_raw(X,i,weight1)\n",
    "    hactivated = tanh(hidden_layer_raw)\n",
    "    return hactivated, weight1\n",
    "\n",
    "'''This function now activates the output layer by applying the sigmoid activation function and returns the \n",
    "weight that was used to get from the hidden layer to the output layer'''\n",
    "\n",
    "def o_activated(X,i, weight1=None, weight2=None):\n",
    "    oraw, weight2 = o_raw(X,i, weight1=weight1, weight2=weight2)\n",
    "    oactivated = tanh(oraw)\n",
    "    return oactivated, weight2\n",
    "\n",
    "'''First Step in Backpropogation, this produces the Gradient Matrix from the Hidden Layer to the\n",
    "Output Layer (8x3)'''\n",
    "\n",
    "def grad_hidden_to_output(X,y,i, oactivated, hactivated): \n",
    "    \n",
    "    E = (oactivated - y[i]) * (1 - oactivated**2)\n",
    "    h_activated_transpose = np.array([[i] for i in hactivated])\n",
    "    grad_hidden_to_output = h_activated_transpose * E \n",
    "    \n",
    "    bias_gradient = ((oactivated - y[i]) * (1 - oactivated**2)).sum()\n",
    "    bias_h2o =  np.array([bias_gradient])\n",
    "    len_bias_row = grad_hidden_to_output.shape[1]\n",
    "    BG_H_O = np.repeat(bias_h2o[0],len_bias_row)\n",
    "    hidden_to_output_gradient_matrix = np.vstack([grad_hidden_to_output, BG_H_O])\n",
    "    return hidden_to_output_gradient_matrix\n",
    "\n",
    "'''The Gradient Matrix from the Input Layer to the Hidden Layer (8x7)'''\n",
    "\n",
    "def grad_input_to_hidden(X, y, i, oactivated, hactivated, W_output):\n",
    "    input_layer = X[i]\n",
    "    target_layer = y[i]\n",
    "    W_output_wo_bias = W_output[:-1,:] \n",
    "    E = (oactivated - target_layer) * (1 - oactivated**2) \n",
    "    grad_input_to_hidden = np.zeros((7,7))\n",
    "    for j in range(len(hactivated)):\n",
    "        pt1 = []  \n",
    "        pt2 = []\n",
    "        pt1.append( np.dot(E, W_output_wo_bias[j]) ) \n",
    "        pt2.append( hactivated[j] * (1 - hactivated[j])) \n",
    "        grad_input_to_hidden[:,j] = np.array(pt1) * np.array(pt2) * input_layer\n",
    "    pt3 = np.dot(E, W_output_wo_bias.T)\n",
    "    pt4 = hactivated *(1-hactivated)\n",
    "    bias_gradient_il = np.dot(pt3,pt4)\n",
    "    len_bias_row = hactivated.shape[0]\n",
    "    BG_I_H = np.repeat(bias_gradient_il, len_bias_row)\n",
    "    input_to_hidden_gradient_matrix = np.vstack((grad_input_to_hidden, BG_I_H))\n",
    "    return input_to_hidden_gradient_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_training(X, y, lr, threshold=.001, epoch_count=1000):\n",
    "    wt1 = None\n",
    "    wt2 = None\n",
    "    final_output = []\n",
    "    errors = [] \n",
    "    for n in range(epoch_count):\n",
    "        M = np.concatenate((X, y), axis=1)\n",
    "        M = pd.DataFrame(M)\n",
    "        M1 = M.sample(frac=1).reset_index(drop=True)\n",
    "        XX_train = np.array(M1.iloc[:,:7])\n",
    "        yy_train = np.array(M1.iloc[:,7:])\n",
    "        avg_error = []       \n",
    "        for i in range(XX_train.shape[0]):  \n",
    "            oact, hact, err, wt1, wt2 = feedforward(XX_train,yy_train,i,weight1=wt1, weight2=wt2)\n",
    "            err, oactivated1, updated_weight1, updated_weight2 = backpropogation(XX_train,yy_train,i,lr,oactivated=oact,hactivated=hact,weight1=wt1, weight2=wt2)\n",
    "            wt1 = updated_weight1\n",
    "            wt2 = updated_weight2\n",
    "            final_output.append(oactivated1)\n",
    "            errors.append(err)\n",
    "            avg_error.append(err)\n",
    "    epoch_errors = np.reshape(errors,(epoch_count, len(X)))\n",
    "    normalized_errors = np.array(list(map(sum,  epoch_errors))) / len( epoch_errors[0])\n",
    "    error_plotter(epoch_errors,threshold)\n",
    "    epochs_mean_convergence = epochs_for_mean_convergence(normalized_errors, threshold)\n",
    "    output = np.asarray(final_output[-len(X):])\n",
    "    encoded_output = output_encoder(output)      \n",
    "    tot = np.sum(np.all(encoded_output == yy_train, axis=1))\n",
    "    accuracy = tot/len(y)\n",
    "    print('After {} epochs, the accuracy is {}%'.format(epoch_count,round(accuracy*100, 2)))\n",
    "    print('Final Error = {}'.format(round(normalized_errors[-1],5)))\n",
    "    print('The Number of Epochs for the Mean Epoch Error to dip below the threshold is {}'.format(epochs_mean_convergence))\n",
    "    return wt1, wt2\n",
    "\n",
    "def nn_testing(X, y,wt1,wt2):\n",
    "    final_output = []\n",
    "    errors = [] \n",
    "    for i in range(X.shape[0]):  \n",
    "        oact, hact, err, wt1, wt2 = feedforward(X,y,i,weight1=wt1, weight2=wt2)\n",
    "        final_output.append(oact)\n",
    "        errors.append(err)\n",
    "    output = np.asarray(final_output[-len(X):])\n",
    "    encoded_output = output_encoder(output)      \n",
    "    tot = np.sum(np.all(encoded_output == y_test, axis=1))\n",
    "    accuracy = tot/len(y_test)\n",
    "    print('Accuracy using updated weights from training set with is {}%'.format(round(accuracy*100, 2)))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2500 epochs, the accuracy is 91.33%\n",
      "Final Error = 0.17326\n",
      "The Number of Epochs for the Mean Epoch Error to dip below the threshold is None\n",
      "Accuracy using updated weights from training set with is 26.67%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wt1, wt2 = nn_training(X_train, y_train, .1, threshold=.01, epoch_count=50)\n",
    "nn_testing(X_test, y_test, wt1, wt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```The results of changing the activation funtion to Tanh() shows that he model overfitts and does not do well on the validation set. Even though it has a 91% accuracy for training, the testing accuracy is 26.67% and looking at the graph, the mean epoch error does not reach the threshold, instead it oscillates randomly around 0.17```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  EXPERIMENT 3: Different Architectures  again using the Sigmoid Activation Function, learning rate = 0.1 & threshold = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Reinstantiate the functions and chave the architectures to determine the results```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Activation Function for this network'''\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "'''Append the entire matrix with a column of ones for the bias node'''\n",
    "\n",
    "def append(X):\n",
    "    height = X.shape[0] \n",
    "    bias_column = np.ones((height,1))\n",
    "    X_appended = np.hstack((X,bias_column))\n",
    "    return X_appended\n",
    "\n",
    "'''This is the dot product between a weight matrix and the appended X matrix (one row at a time), this returns\n",
    "an unactivated hidden layer of length 7, as specified by the dimensions of the problem'''\n",
    "\n",
    "def h_raw(X,i,weight1=None): \n",
    "    if weight1 is None:\n",
    "        weight1 = w_input(X)\n",
    "    X_appended_row = append(X)[i]\n",
    "    hraw = np.dot(X_appended_row, weight1)\n",
    "    return hraw, weight1\n",
    "\n",
    "'''This function now activates the hidden layer by applying the sigmoid activation function and returns the \n",
    "weight that was used to get from the input layer to the hidden layer'''\n",
    "\n",
    "def h_activated(X,i,weight1=None):\n",
    "    hidden_layer_raw, weight1 = h_raw(X,i,weight1)\n",
    "    hactivated = sigmoid(hidden_layer_raw)\n",
    "    return hactivated, weight1\n",
    "\n",
    "'''Initializes a Weight Matrix for the hidden to output layer using the length of the activated hidden layer,\n",
    "this also appends a random weight at the bottom of the matrix for the bias from hidden to output layer'''\n",
    "\n",
    "def w_output(X, i, y_train, weight1=None):\n",
    "    hactivated, _ = h_activated(X,i,weight1=weight1)\n",
    "    rows = len(hactivated)\n",
    "    W = np.random.sample((rows, 3)) - 0.5\n",
    "    random_weight = np.random.sample([1]) - 0.5\n",
    "    weight_vector = np.repeat(random_weight,3)\n",
    "    W_output = np.insert(W, rows, weight_vector, axis=0)\n",
    "    return W_output\n",
    "\n",
    "'''This is the dot product between an appended activated hidden layer and W_output, this gives us the first\n",
    "output layer values'''\n",
    "\n",
    "def o_raw(X,i, weight1=None, weight2=None):\n",
    "    hactivated, weight1 = h_activated(X, i , weight1)\n",
    "    h_activated_with_bias = np.append(hactivated,[1]) \n",
    "    if weight2 is None:\n",
    "        weight2 = w_output(X,i,weight1)\n",
    "    oraw = np.dot(h_activated_with_bias, weight2)\n",
    "    return oraw, weight2\n",
    "\n",
    "'''This function now activates the output layer by applying the sigmoid activation function and returns the \n",
    "weight that was used to get from the hidden layer to the output layer'''\n",
    "\n",
    "def o_activated(X,i, weight1=None, weight2=None):\n",
    "    oraw, weight2 = o_raw(X,i, weight1=weight1, weight2=weight2)\n",
    "    oactivated = sigmoid(oraw)\n",
    "    return oactivated, weight2\n",
    "\n",
    "'''This Error function determines the error of the output for the i-th row of the Matrix and the i-th target layer\n",
    "and outputs a single value that will be used in the backpropogation steps'''\n",
    "\n",
    "def error(X,y,i,weight1=None, weight2=None):\n",
    "    oactivated, _ = o_activated(X, i, weight1=weight1, weight2=weight2)\n",
    "    error = .5*((y[i] - oactivated)**2).sum()\n",
    "    return error\n",
    "\n",
    "'''The Feedforward Steps all conglomerated into one function from X matrix row to output activated. This catch-all\n",
    "function will return the outputlayer actived for the i-th row, the weight matricies used form both the input layer\n",
    "to hidden layer and from the hidden layer to the input layer and the associated erro'''\n",
    "\n",
    "def feedforward(X, y, i, weight1=None, weight2=None):\n",
    "    hactivated, weight1 = h_activated(X,i, weight1)\n",
    "    oactivated, weight2 = o_activated(X,i, weight1=weight1, weight2=weight2)\n",
    "    err = error(X,y,i,weight1=weight1, weight2=weight2)\n",
    "    return oactivated, hactivated, err, weight1, weight2\n",
    "\n",
    "'''First Step in Backpropogation, this produces the Gradient Matrix from the Hidden Layer to the\n",
    "Output Layer (8x3)'''\n",
    "\n",
    "def grad_hidden_to_output(X,y,i, oactivated, hactivated): \n",
    "    E = (oactivated - y[i]) * oactivated * (1 - oactivated)\n",
    "    h_activated_transpose = np.array([[i] for i in hactivated])\n",
    "    grad_hidden_to_output = h_activated_transpose * E \n",
    "    bias_gradient = ((oactivated - y[i]) * oactivated * (1 - oactivated)).sum()\n",
    "    bias_h2o =  np.array([bias_gradient])\n",
    "    len_bias_row = grad_hidden_to_output.shape[1]\n",
    "    BG_H_O = np.repeat(bias_h2o[0],len_bias_row)\n",
    "    hidden_to_output_gradient_matrix = np.vstack([grad_hidden_to_output, BG_H_O])\n",
    "    return hidden_to_output_gradient_matrix\n",
    "\n",
    "'''Updated Weights from the Input to Hidden Layer with learning rate lr'''\n",
    "\n",
    "def update_input_hidden_weights(X,y,i,lr, oactivated, hactivated, weight1=None, weight2=None):\n",
    "    input_to_hidden_gradient_matrix = grad_input_to_hidden(X, y, i, oactivated, hactivated, weight2)\n",
    "    updated_weights_input_hidden = weight1 - lr * input_to_hidden_gradient_matrix\n",
    "    return updated_weights_input_hidden\n",
    "\n",
    "'''Updated Weights for Hidden to Output Layer with lr'''\n",
    "\n",
    "def update_hidden_output_weights(X,y,i,lr, oactivated, hactivated, weight2=None):\n",
    "    hidden_to_output_gradient_matrix =  grad_hidden_to_output(X,y,i, oactivated, hactivated)\n",
    "    updated_weights_hidden_output = weight2 - lr * hidden_to_output_gradient_matrix\n",
    "    return updated_weights_hidden_output\n",
    "\n",
    "'''Backpropogation Function'''\n",
    "\n",
    "def backpropogation(X,y,i,lr,oactivated,hactivated,weight1=None, weight2=None):\n",
    "    updated_weight1 = update_input_hidden_weights(X,y,i,lr, oactivated, hactivated, weight1=weight1, weight2=weight2)\n",
    "    updated_weight2 = update_hidden_output_weights(X,y,i,lr, oactivated, hactivated, weight2)\n",
    "    oactivated1, hactivated1, err, updated_weight1, updated_weight2 = feedforward(X, y, i, weight1=updated_weight1, weight2=updated_weight2)\n",
    "    return err, oactivated1, updated_weight1, updated_weight2\n",
    "\n",
    "'''Transforms the Rows of all oactivated1 to one hot encoding to compare with y_trian'''\n",
    "\n",
    "def output_encoder(Matrix):\n",
    "    for row in Matrix:\n",
    "        row[row.argmax(0)] = 1\n",
    "        row[row < 1] = 0\n",
    "    return Matrix\n",
    "\n",
    "'''Creates a visualization of the Errors'''\n",
    "\n",
    "def error_plotter(Matrix,threshold): \n",
    "    result = np.array(list(map(sum, Matrix))) / len(Matrix[0])\n",
    "    index = np.array(list(range(len(result))))\n",
    "    Ezz = np.vstack((index+1, result)).T\n",
    "    x = Ezz[:,0]\n",
    "    y = Ezz[:,1]\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(x,y)\n",
    "    plt.axhline(y=threshold, color='r', linestyle='--')\n",
    "    plt.show\n",
    "\n",
    "'''This function returns the number of EPOCHS needed to reach convergence to the error threshold'''\n",
    "    \n",
    "def epochs_for_convergence(avg_error, threshold, X):\n",
    "    for i in range(len(avg_error)):\n",
    "        if avg_error[i] < threshold:\n",
    "            count = i / len(X)\n",
    "            return math.ceil(count)\n",
    "        \n",
    "def epochs_for_mean_convergence(normalized_error, threshold):\n",
    "    for i in range(len(normalized_error)):\n",
    "        if normalized_error[i] < threshold:\n",
    "            count = i\n",
    "            return math.ceil(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [7,9,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Initializes a Weight Matrix from the input layer to the hidden layer using the length of a single row \n",
    "for argument Matrix X'''\n",
    "\n",
    "def w_input(X):\n",
    "    X_appended = append(X)\n",
    "    rows = len(X[0])+2 \n",
    "    cols = len(X[0])\n",
    "    W = np.random.sample((cols, rows)) - 0.5\n",
    "    random_bias_weight = np.random.sample()\n",
    "    bias_row = np.repeat(random_bias_weight, rows)\n",
    "    W_input = np.insert(W, cols, bias_row, axis=0)\n",
    "    return W_input\n",
    "\n",
    "'''The Gradient Matrix from the Input Layer to the Hidden Layer (8x7)'''\n",
    "\n",
    "def grad_input_to_hidden(X, y, i, oactivated, hactivated, W_output):\n",
    "    input_layer = X[i]\n",
    "    target_layer = y[i]\n",
    "    W_output_wo_bias = W_output[:-1,:] \n",
    "    E = (oactivated - target_layer) * oactivated * (1 - oactivated) \n",
    "    grad_input_to_hidden = np.zeros((7,9))\n",
    "    for j in range(len(hactivated)):\n",
    "        pt1 = []  \n",
    "        pt2 = []\n",
    "        pt1.append( np.dot(E, W_output_wo_bias[j]) ) \n",
    "        pt2.append( hactivated[j] * (1 - hactivated[j])) \n",
    "        grad_input_to_hidden[:,j] = np.array(pt1) * np.array(pt2) * input_layer\n",
    "    pt3 = np.dot(E, W_output_wo_bias.T)\n",
    "    pt4 = hactivated *(1-hactivated)\n",
    "    bias_gradient_il = np.dot(pt3,pt4)\n",
    "    len_bias_row = hactivated.shape[0]\n",
    "    BG_I_H = np.repeat(bias_gradient_il, len_bias_row)\n",
    "    input_to_hidden_gradient_matrix = np.vstack((grad_input_to_hidden, BG_I_H))\n",
    "    return input_to_hidden_gradient_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2500 epochs, the accuracy is 100.0%\n",
      "Final Error = 0.00275\n",
      "The Number of Epochs for the Mean Epoch Error to dip below the threshold is 1222\n",
      "Accuracy using updated weights from training set with is 96.67%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wt1, wt2 = nn_training(X_train, y_train, .1, threshold=.01, epoch_count=2500)\n",
    "nn_testing(X_test, y_test, wt1, wt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```This larger hidden layer works better on all metrics than our original [7,7,3]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [7,3,3] Archiceture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Initializes a Weight Matrix from the input layer to the hidden layer using the length of a single row \n",
    "for argument Matrix X'''\n",
    "\n",
    "def w_input(X):\n",
    "    X_appended = append(X)\n",
    "    rows = len(X[0])-4 \n",
    "    cols = len(X[0])\n",
    "    W = np.random.sample((cols, rows)) - 0.5\n",
    "    random_bias_weight = np.random.sample()\n",
    "    bias_row = np.repeat(random_bias_weight, rows)\n",
    "    W_input = np.insert(W, cols, bias_row, axis=0)\n",
    "    return W_input\n",
    "\n",
    "'''The Gradient Matrix from the Input Layer to the Hidden Layer (8x7)'''\n",
    "\n",
    "def grad_input_to_hidden(X, y, i, oactivated, hactivated, W_output):\n",
    "    input_layer = X[i]\n",
    "    target_layer = y[i]\n",
    "    W_output_wo_bias = W_output[:-1,:] \n",
    "    E = (oactivated - target_layer) * oactivated * (1 - oactivated) \n",
    "    grad_input_to_hidden = np.zeros((7,3))\n",
    "    for j in range(len(hactivated)):\n",
    "        pt1 = []  \n",
    "        pt2 = []\n",
    "        pt1.append( np.dot(E, W_output_wo_bias[j]) ) \n",
    "        pt2.append( hactivated[j] * (1 - hactivated[j])) \n",
    "        grad_input_to_hidden[:,j] = np.array(pt1) * np.array(pt2) * input_layer\n",
    "    pt3 = np.dot(E, W_output_wo_bias.T)\n",
    "    pt4 = hactivated *(1-hactivated)\n",
    "    bias_gradient_il = np.dot(pt3,pt4)\n",
    "    len_bias_row = hactivated.shape[0]\n",
    "    BG_I_H = np.repeat(bias_gradient_il, len_bias_row)\n",
    "    input_to_hidden_gradient_matrix = np.vstack((grad_input_to_hidden, BG_I_H))\n",
    "    return input_to_hidden_gradient_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2500 epochs, the accuracy is 99.33%\n",
      "Final Error = 0.01068\n",
      "The Number of Epochs for the Mean Epoch Error to dip below the threshold is None\n",
      "Accuracy using updated weights from training set with is 96.67%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wt1, wt2 = nn_training(X_train, y_train, .1, threshold=.01, epoch_count=2500)\n",
    "nn_testing(X_test, y_test, wt1, wt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```The training and testing accuracies is great but the Epoch mean error never dips below the error threshold```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [7,5,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Initializes a Weight Matrix from the input layer to the hidden layer using the length of a single row \n",
    "for argument Matrix X'''\n",
    "\n",
    "def w_input(X):\n",
    "    X_appended = append(X)\n",
    "    rows = len(X[0])-2\n",
    "    cols = len(X[0])\n",
    "    W = np.random.sample((cols, rows)) - 0.5\n",
    "    random_bias_weight = np.random.sample()\n",
    "    bias_row = np.repeat(random_bias_weight, rows)\n",
    "    W_input = np.insert(W, cols, bias_row, axis=0)\n",
    "    return W_input\n",
    "\n",
    "'''The Gradient Matrix from the Input Layer to the Hidden Layer (8x7)'''\n",
    "\n",
    "def grad_input_to_hidden(X, y, i, oactivated, hactivated, W_output):\n",
    "    input_layer = X[i]\n",
    "    target_layer = y[i]\n",
    "    W_output_wo_bias = W_output[:-1,:] \n",
    "    E = (oactivated - target_layer) * oactivated * (1 - oactivated) \n",
    "    grad_input_to_hidden = np.zeros((7,5))\n",
    "    for j in range(len(hactivated)):\n",
    "        pt1 = []  \n",
    "        pt2 = []\n",
    "        pt1.append( np.dot(E, W_output_wo_bias[j]) ) \n",
    "        pt2.append( hactivated[j] * (1 - hactivated[j])) \n",
    "        grad_input_to_hidden[:,j] = np.array(pt1) * np.array(pt2) * input_layer\n",
    "    pt3 = np.dot(E, W_output_wo_bias.T)\n",
    "    pt4 = hactivated *(1-hactivated)\n",
    "    bias_gradient_il = np.dot(pt3,pt4)\n",
    "    len_bias_row = hactivated.shape[0]\n",
    "    BG_I_H = np.repeat(bias_gradient_il, len_bias_row)\n",
    "    input_to_hidden_gradient_matrix = np.vstack((grad_input_to_hidden, BG_I_H))\n",
    "    return input_to_hidden_gradient_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2500 epochs, the accuracy is 99.33%\n",
      "Final Error = 0.00894\n",
      "The Number of Epochs for the Mean Epoch Error to dip below the threshold is 2172\n",
      "Accuracy using updated weights from training set with is 95.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wt1, wt2 = nn_training(X_train, y_train, .1, threshold=.01, epoch_count=2500)\n",
    "nn_testing(X_test, y_test, wt1, wt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```This Architecture is very good. This is about as good as [7,9,3]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
